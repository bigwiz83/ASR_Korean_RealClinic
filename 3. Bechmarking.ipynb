{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Korean ASR with Riva"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Refer to: https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Korean ASR models from NGC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Acoustic models from NGC.\n",
    "# Note that\n",
    "#     1) NGC API is installed.\n",
    "#     2) The directory where being downloaded must have \"write\" authority.\n",
    "#     3) NGC CLI should be executed at the location where models are going to be downloaded.(i.e., at ./models/korean)\n",
    "#     4) It's OK to execute the NGC CLI outside the container(i.e., local workstation). However, make sure that the downloaded directory should be mounted to the container so that you can access to the models inside the container.\n",
    "\n",
    "# Conformer-CTC\n",
    "import numpy as np\n",
    "!ngc registry model download-version \"nvidia/tao/speechtotext_ko_kr_conformer:deployable_v1.0\"\n",
    "# Citrinet-1024\n",
    "!ngc registry model download-version \"nvidia/tao/speechtotext_ko_kr_citrinet:deployable_v1.0\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# N-gram decoder models from NGC\n",
    "# N-Gram\n",
    "!ngc registry model download-version \"nvidia/tao/speechtotext_ko_kr_lm:deployable_v1.0\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Punctuation(optional)\n",
    "!ngc registry model download-version \"nvidia/tao/punctuationcapitalization_ko_kr_bert_base:deployable_v1.1\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get install -y libsndfile1-dev\n",
    "!pip install librosa\n",
    "!pip install datasets[audio]\n",
    "!pip install openai\n",
    "!pip install -U openai-whisper\n",
    "!pip install triton client\n",
    "!pip install tritonclient\n",
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-eff\n",
    "!pip install nemo2riva\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get install -y libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 python3-pyaudio\n",
    "!pip install pyaudio\n",
    "# For some errors: https://stackoverflow.com/questions/59006083/how-to-install-portaudio-on-pi-properly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cd ../.."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and deploy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's deploy Korean Citrinet-1024 as an example. The detailed pipeline configurations are specified in https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-pipeline-configuration.html.\n",
    "1. Launch Riva Servicemaker **at your workstation**.\n",
    "    - `./scripts/build_deploy/riva_servicemaker.sh`\n",
    "        - docker run --init -it --rm --gpus  all -v riva-model-repo:/data -v E:\\기본연구_Data\\riva_quickstart_v2.8.1\\riva_demo:/servicemaker-dev --name riva-service-maker nvcr.io/nvidia/riva/riva-speech:2.8.1-servicemaker /bin/bash\n",
    "                -\n",
    "2. Build rmir file **inside the servicemaker container**.\n",
    "    - `cd /servicemaker-dev`\n",
    "    - `./scripts/build_deploy/korean_models/riva_asr_citrinet_kr_build.sh`\n",
    "    - This shell script consists of like:\n",
    "        ```sh\n",
    "        riva-build speech-recognition \\\n",
    "            /servicemaker-dev/<rmir_filename>:<encryption_key> \\\n",
    "            /servicemaker-dev/<riva_filename>:<encryption_key> \\\n",
    "            --name=<pipeline_name>\n",
    "            --decoder_type=flashlight \\\n",
    "            --decoding_language_model_binary=<KENLM_binary_filename> \\\n",
    "            --decoding_vocab=<decoder_vocab_file>\n",
    "            ...\n",
    "        ```\n",
    "3. Deploy models **inside the servicemaker container**.\n",
    "    - `./scripts/build_deploy/korean_models/riva_asr_citrinet_kr_deploy.sh`\n",
    "\n",
    "You can also deploy Conformer-CTC for Korean with the same procedure using those scripts:\n",
    "- `./scripts/build_deploy/korean_models/riva_asr_conformer_kr_build.sh`\n",
    "- `./scripts/build_deploy/korean_models/riva_asr_conformer_kr_deploy.sh`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For reference, **building and deploying each model takes approximately over 30 mins(Citrinet-1024 takes especially much longer), respectively.** After the deployment is done, restart the riva server at your local workstation.\n",
    "```bash\n",
    "bash resources/riva_quickstart_v2.8.1/riva_stop.sh\n",
    "bash resources/riva_quickstart_v2.8.1/riva_start.sh\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check whether your model is successfully deployed using Trtion APIs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "# docker ps 로 8001에 매핑되는 주소를 매번 바꿔 적어줘야 함.\n",
    "#trt_channel = grpc.insecure_channel(\"riva-speech:57876\")\n",
    "trt_channel = grpc.insecure_channel(\"localhost:8001\")\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(trt_channel)\n",
    "\n",
    "try:\n",
    "    request = service_pb2.ServerLiveRequest()\n",
    "    response = grpc_stub.ServerLive(request)\n",
    "    print(\"server {}\".format(response))\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "request = service_pb2.RepositoryIndexRequest()\n",
    "response = grpc_stub.RepositoryIndex(request)\n",
    "\n",
    "print(\"num models: {}\\n\".format(len(response.models)))\n",
    "print([i for i in response.models])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Offline test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import io, os\n",
    "import librosa\n",
    "import riva.client\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "# Create Riva clients and connect to Riva Speech API server\n",
    "auth =riva.client.Auth(uri=\"localhost:50051\")\n",
    "nemo_dir = os.path.join('E:/기본연구_Data')\n",
    "\n",
    "#server\n",
    "riva_asr = riva.client.ASRService(auth)\n",
    "riva_nlp = riva.client.NLPService(auth)\n",
    "# riva_tts = riva.client.SpeechSynthesisService(auth)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "preds = []\n",
    "refs = []\n",
    "with open(f\"{nemo_dir}/Validation_RealAudio/Manifests/test_merged.json\", 'r') as inpufile:\n",
    "    for line in tqdm(inpufile):\n",
    "        sample = json.loads(line.strip())\n",
    "\n",
    "        audio, sr = librosa.core.load(sample['audio_filepath'], sr=None)\n",
    "        with io.open(sample['audio_filepath'], 'rb') as fh:\n",
    "            content = fh.read()\n",
    "\n",
    "        offline_config = riva.client.RecognitionConfig(\n",
    "            encoding=riva.client.AudioEncoding.LINEAR_PCM,                     # Supports LINEAR_PCM, FLAC, MULAW and ALAW audio encodings\n",
    "            sample_rate_hertz = sr,                                            # Audio will be resampled if necessary\n",
    "            max_alternatives=1,                                                # How many top-N hypotheses to return\n",
    "            enable_automatic_punctuation=False,                                 # Add punctuation when end of VAD detected\n",
    "            audio_channel_count = 1,                                           # Mono channel\"\n",
    "            verbatim_transcripts=True,\n",
    "            # model=\"jbs_quartznet_kr_E500\"                   #  In the case where multiple models might be able to fulfill the client request, one model is selected at random. Y\n",
    "            model=\"woojin-conformer-ko-KR-asr-streaming\",\n",
    "            # model=\"woojin-citrinet-1024-ko-KR-asr-streaming\",\n",
    "        )\n",
    "        response = riva_asr.offline_recognize(content, offline_config)\n",
    "        try:\n",
    "            asr_best_transcript = response.results[0].alternatives[0].transcript\n",
    "            preds.append(asr_best_transcript)\n",
    "        except IndexError as e:\n",
    "            preds.append(\"\")\n",
    "            refs.append(sample['text'])\n",
    "            continue\n",
    "        refs.append(sample['text'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = \"../../Validation_RealAudio/audio/DOC/20220805_1_DOC_058.wav\"\n",
    "with io.open(path, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "ipd.Audio(path)\n",
    "\n",
    "response = riva_asr.offline_recognize(content, offline_config)\n",
    "asr_best_transcript = response.results[0].alternatives[0].transcript\n",
    "print(\"ASR Transcript:\", asr_best_transcript)\n",
    "\n",
    "print(\"\\n\\nFull Response Message:\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import pandas as pd\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "hypotheses = []\n",
    "references = []\n",
    "data = pd.DataFrame(dict(hypothesis=preds, reference=refs))\n",
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in preds]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in refs]\n",
    "data.to_csv('result-conformer.csv')\n",
    "\n",
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "cer = jiwer.cer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "print(f\"WER: {wer * 100:.3f} %\")\n",
    "print(f\"CER: {cer * 100:.3f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Punctuation model test(optional)\n",
    "model_name = \"woojin-punctuation-KR\"\n",
    "response = riva_nlp.transform_text(input_strings=asr_best_transcript, model_name=model_name)\n",
    "\n",
    "print(\"Transformed results are:\")\n",
    "print(\"\\n\".join([i for i in response.text]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whisper Benchmark\n",
    "https://github.com/openai/whisper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Bingsu/zeroth-korean\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "import whisper\n",
    "model = whisper.load_model(\"large\")\n",
    "options = whisper.DecodingOptions(language=\"ko\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds_w = []\n",
    "refs_w = []\n",
    "with open(f\"{nemo_dir}/Validation_RealAudio/Manifests/test_merged.json\", 'r') as inpufile:\n",
    "    for line in tqdm(inpufile):\n",
    "        sample = json.loads(line.strip())\n",
    "\n",
    "        audio = whisper.load_audio(sample['audio_filepath'])\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "        result = whisper.decode(model, mel, options)\n",
    "        preds_w.append(result.text)\n",
    "        refs_w.append(sample['text'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import pandas as pd\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "hypotheses = []\n",
    "references = []\n",
    "data = pd.DataFrame(dict(hypothesis=preds_w, reference=refs_w))\n",
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in preds_w]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in refs_w]\n",
    "data.to_csv('result-whisper-large.csv')\n",
    "\n",
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "cer = jiwer.cer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "print(f\"WER: {wer * 100:.3f} %\")\n",
    "print(f\"CER: {cer * 100:.3f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### E100 model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "cdd488b9-ae5f-4bf1-9ba5-b5e9f5821ebf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad88ca6-7653-4273-95c7-222b3650f069",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "preds_j = []\n",
    "refs_j = []\n",
    "audiofiles = []\n",
    "#model_j = nemo_asr.models.EncDecCTCModel.restore_from(f\"{nemo_dir}/ASR_Nemo_Results/A6000/Model-ko-epoch 500-00-59-29-January-from-RealAudio.nemo\")\n",
    "\n",
    "model_j = nemo_asr.models.EncDecCTCModel.restore_from(f\"{nemo_dir}/ASR_Nemo_Results/A6000/Model-ko-epoch 100-04-12-06-January-from-RealAudio.nemo\")\n",
    "\n",
    "#model_j = nemo_asr.models.EncDecCTCModel.restore_from(f\"{nemo_dir}/ASR_Nemo_Results/A6000/Model-ko-epoch 150-06-40-07-January-from-RealAudio.nemo\")\n",
    "\n",
    "with open(f\"{nemo_dir}/Validation_RealAudio/Manifests/test_merged.json\", 'r') as inpufile:\n",
    "    for line in tqdm(inpufile):\n",
    "        sample = json.loads(line.strip())\n",
    "        audiofiles.append(sample['audio_filepath'])\n",
    "        refs_j.append(sample['text'])\n",
    "\n",
    "preds_j = model_j.transcribe(paths2audio_files=audiofiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import pandas as pd\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "hypotheses = []\n",
    "references = []\n",
    "data = pd.DataFrame(dict(hypothesis=preds_j, reference=refs_j))\n",
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in preds_j]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in refs_j]\n",
    "data.to_csv('result-finetuned-E100.csv')\n",
    "\n",
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "cer = jiwer.cer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "print(f\"WER: {wer * 100:.3f} %\")\n",
    "print(f\"CER: {cer * 100:.3f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## zeroth-korean\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Bingsu/zeroth-korean\")\n",
    "dataset[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"large\")\n",
    "options = whisper.DecodingOptions(language=\"ko\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "preds_z = []\n",
    "refs_z = []\n",
    "\n",
    "for k in tqdm(dataset['test']):\n",
    "    preds_z.append(whisper.transcribe(model, np.float32(k['audio']['array']), language=\"ko\"))\n",
    "    refs_z.append(k['text'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import pandas as pd\n",
    "from whisper.normalizers import BasicTextNormalizer\n",
    "\n",
    "preds_z2 = []\n",
    "for k in preds_z:\n",
    "    preds_z2.append(k['text'])\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "hypotheses = []\n",
    "references = []\n",
    "data = pd.DataFrame(dict(hypothesis=preds_z2, reference=refs_z))\n",
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in preds_z2]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in refs_z]\n",
    "data.to_csv('result-whisper-zeroth-testset.csv')\n",
    "\n",
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "cer = jiwer.cer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "print(f\"WER: {wer * 100:.3f} %\")\n",
    "print(f\"CER: {cer * 100:.3f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}