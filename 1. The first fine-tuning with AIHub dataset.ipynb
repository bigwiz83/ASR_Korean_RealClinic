{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGV_ioUHqhun",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n",
    "\"\"\"\n",
    "\n",
    "# Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg libsox-fmt-mp3\n",
    "!pip install unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
    "that you want to use the \"Run All Cells\" (or similar) option.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt-get  install -y sox libsndfile1 ffmpeg libsox-fmt-mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cjMaek4rY8-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "import copy\n",
    "from omegaconf import OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSTb6b5DriWG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "from nemo.utils import logging, exp_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0i8hvt688hc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "VERSION = \"aihub-2022-07\"\n",
    "LANGUAGE = \"ko\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wI16qY_misb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer_dir = os.path.join('tokenizers', LANGUAGE)\n",
    "nemo_dir = os.path.join('/mnt/sdb/jhchang/nemo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!apt-get install  sox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYw2sHWtOh3x",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that the dataset has been downloaded, let's prepare some paths to easily access the manifest files for the train, dev, and test partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7WAGLX59C26",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_manifest = f\"{nemo_dir}/Training/Manifests/korean_train.json\"\n",
    "#dev_manifest = f\"{manifest_dir}/commonvoice_dev_manifest.json\"\n",
    "test_manifest = f\"{nemo_dir}/Validation/Manifests/korean_val.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txKWXZLbrUsU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Manifest utilities\n",
    "\n",
    "First, we construct some utilities to read and write manifest files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdkJYxUirp7C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Manifest Utils\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "def read_manifest(path):\n",
    "    manifest = []\n",
    "    i=1\n",
    "    with open(path, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"Reading manifest data\"):\n",
    "            line = line.replace(\"\\n\", \"\")\n",
    "            data = json.loads(line)\n",
    "            manifest.append(data)\n",
    "\n",
    "    return manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HngfzcwOijy4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_manifest_data = read_manifest(train_manifest)\n",
    "#dev_manifest_data = read_manifest(dev_manifest)\n",
    "test_manifest_data = read_manifest(test_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thrTwcCVra2N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we extract just the text corpus from the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2iwnvhXimfG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_text = [data['text'] for data in train_manifest_data]\n",
    "#dev_text = [data['text'] for data in dev_manifest_data]\n",
    "test_text = [data['text'] for data in test_manifest_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fDoQQwyrhkV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Character set\n",
    "\n",
    "Let us calculate the character set - which is the set of unique tokens that exist within the text manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpUb_pI5imhh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_charset(manifest_data):\n",
    "    charset = defaultdict(int)\n",
    "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
    "        text = row['text']\n",
    "        for character in text:\n",
    "            charset[character] += 1\n",
    "    return charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obcPlrOJimju",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_charset = get_charset(train_manifest_data)\n",
    "#dev_charset = get_charset(dev_manifest_data)\n",
    "test_charset = get_charset(test_manifest_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWmf3aNYi7on",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Count the number of unique tokens that exist within this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8QVdph6imlz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dev_set = set.union(set(train_charset.keys()))\n",
    "#, set(dev_charset.keys()))\n",
    "test_set = set(test_charset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgCfETWNimn3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of tokens in train+dev set : {len(train_dev_set)}\")\n",
    "print(f\"Number of tokens in test set : {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWfJJHefjGbS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Count number of Out-Of-Vocabulary tokens in the test set\n",
    "\n",
    "Given such a vast number of tokens exist in the train and dev set, lets make sure that there are no outlier tokens in the test set (remember: the number of kanji used regularly is roughly more than 2000 tokens!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPrBi35Cimqc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# OOV tokens in test set\n",
    "train_test_common = set.intersection(train_dev_set, test_set)\n",
    "test_oov = test_set - train_test_common\n",
    "print(f\"Number of OOV tokens in test set : {len(test_oov)}\")\n",
    "print()\n",
    "print(test_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDDiXCiPimr_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Populate dictionary mapping count: list[tokens]\n",
    "train_counts = defaultdict(list)\n",
    "for token, count in train_charset.items():\n",
    "    train_counts[count].append(token)\n",
    "for token, count in test_charset.items():\n",
    "    train_counts[count].append(token)\n",
    "\n",
    "# Compute sorter order of the count keys\n",
    "count_keys = sorted(list(train_counts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oni2J47zeE5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Build a paired list that computes the number of unique kanji which occurs less than some `MAX_COUNT` number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJeVEKvAimwE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MAX_COUNT = 32\n",
    "\n",
    "TOKEN_COUNT_X = []\n",
    "NUM_TOKENS_Y = []\n",
    "for count in range(1, MAX_COUNT + 1):\n",
    "    if count in train_counts:\n",
    "        num_tokens = len(train_counts[count])\n",
    "\n",
    "        TOKEN_COUNT_X.append(count)\n",
    "        NUM_TOKENS_Y.append(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKULANgINqbq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(x=TOKEN_COUNT_X, height=NUM_TOKENS_Y)\n",
    "plt.title(\"Occurance of unique tokens in train+dev set\")\n",
    "plt.xlabel(\"# of occurances\")\n",
    "plt.ylabel(\"# of tokens\")\n",
    "plt.xlim(0, MAX_COUNT);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9G6laS0ojV-B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNCOMMON_TOKENS_COUNT = 5\n",
    "\n",
    "chars_with_infrequent_occurance = set()\n",
    "for count in range(1, UNCOMMON_TOKENS_COUNT + 1):\n",
    "    if count in train_counts:\n",
    "        token_list = train_counts[count]\n",
    "        chars_with_infrequent_occurance.update(set(token_list))\n",
    "\n",
    "print(f\"Number of tokens with <= {UNCOMMON_TOKENS_COUNT} occurances : {len(chars_with_infrequent_occurance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gZSbBXZjhXa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Remove Out-of-Vocabulary tokens from the test set\n",
    "\n",
    "Previously we counted the set of Out-of-Vocabulary tokens that exist in the test set but not in the train or dev set. Now, let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnh_pnL2jWAY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_tokens = set.union(train_dev_set, test_set)\n",
    "print(f\"Original train+dev+test vocab size : {len(all_tokens)}\")\n",
    "\n",
    "extra_kanji = set(test_oov)\n",
    "train_token_set = all_tokens - extra_kanji\n",
    "print(f\"New train vocab size : {len(train_token_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kaX9WzK15Q6t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Dakuten normalization\n",
    "perform_dakuten_normalization = False #@param [\"True\", \"False\"] {type:\"raw\"}\n",
    "PERFORM_DAKUTEN_NORMALIZATION = bool(perform_dakuten_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiEZVEshOp-y",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def process_dakuten(text):\n",
    "    normalized_text = unicodedata.normalize('NFD', text)\n",
    "    normalized_text = normalized_text.replace(\"\\u3099\", \"\").replace(\"\\u309A\", \"\")\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV4kOgpvjWGg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PERFORM_DAKUTEN_NORMALIZATION:\n",
    "    normalized_train_token_set = set()\n",
    "    for token in train_token_set:\n",
    "        normalized_token = process_dakuten(str(token))\n",
    "        normalized_train_token_set.update(normalized_token)\n",
    "        \n",
    "    print(f\"After dakuten normalization, number of train tokens : {len(normalized_train_token_set)}\")\n",
    "else:\n",
    "    normalized_train_token_set = train_token_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN3asqvsrp_S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing steps\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\…\\{\\}\\【\\】\\・\\。\\『\\』\\、\\ー\\〜]'  # remove special character tokens\n",
    "#kanji_removal_regex = '[' + \"\".join([f\"\\{token}\" for token in extra_kanji]) + ']'  # remove test set kanji\n",
    "\n",
    "\n",
    "def remove_special_characters(data):\n",
    "    data[\"text\"] = re.sub(chars_to_ignore_regex, '', data[\"text\"]).lower().strip()\n",
    "    return data\n",
    "\n",
    "def remove_extra_kanji(data):\n",
    "    data[\"text\"] = re.sub(kanji_removal_regex, '', data[\"text\"])\n",
    "    return data\n",
    "\n",
    "def remove_dakuten(data):\n",
    "    # perform dakuten normalization (if it was requested)\n",
    "    if PERFORM_DAKUTEN_NORMALIZATION:\n",
    "        text = data['text']\n",
    "        data['text'] = process_dakuten(text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZkvFKBur78c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Process dataset\n",
    "\n",
    "Now that we have the functions necessary to clean up the transcripts, let's create a small pipeline to clean up the manifest and write new manifests for us. For simplicity's sake (as the dataset is so small), a simple sequential pipeline will be sufficient for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwNtHeHLjqJl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Processing pipeline\n",
    "def apply_preprocessors(manifest, preprocessors):\n",
    "    for processor in preprocessors:\n",
    "        for idx in tqdm(range(len(manifest)), desc=f\"Applying {processor.__name__}\"):\n",
    "            manifest[idx] = processor(manifest[idx])\n",
    "\n",
    "    print(\"Finished processing manifest !\")\n",
    "    return manifest\n",
    "\n",
    "def write_processed_manifest(data, original_path):\n",
    "    original_manifest_name = os.path.basename(original_path)\n",
    "    new_manifest_name = original_manifest_name.replace(\".json\", \"_processed.json\")\n",
    "\n",
    "    manifest_dir = os.path.split(original_path)[0]\n",
    "    filepath = os.path.join(manifest_dir, new_manifest_name)\n",
    "    with open(filepath, 'w') as f:\n",
    "        for datum in tqdm(data, desc=\"Writing manifest data\"):\n",
    "            datum = json.dumps(datum)\n",
    "            f.write(f\"{datum}\\n\")\n",
    "    print(f\"Finished writing manifest: {filepath}\")\n",
    "    return filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xB06YHmDr-Ja",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# List of pre-processing functions\n",
    "PREPROCESSORS = [\n",
    "    remove_special_characters\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lqUvpkrr7bQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load manifests\n",
    "train_data = read_manifest(train_manifest)\n",
    "#dev_data = read_manifest(dev_manifest)\n",
    "test_data = read_manifest(test_manifest)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data_processed = apply_preprocessors(train_data, PREPROCESSORS)\n",
    "#dev_data_processed = apply_preprocessors(dev_data, PREPROCESSORS)\n",
    "test_data_processed = apply_preprocessors(test_data, PREPROCESSORS)\n",
    "\n",
    "# Write new manifests\n",
    "#train_manifest_cleaned = write_processed_manifest(train_data_processed, train_manifest)\n",
    "#dev_manifest_cleaned = write_processed_manifest(dev_data_processed, dev_manifest)\n",
    "#test_manifest_cleaned = write_processed_manifest(test_data_processed, test_manifest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_manifest_cleaned = \"/mnt/sdb/jhchang/nemo//Training/Manifests/korean_train_processed.json\"\n",
    "test_manifest_cleaned = \"/mnt/sdb/jhchang/nemo//Validation/Manifests/korean_val_processed.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWDvMDU2O9pV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Final character set\n",
    "\n",
    "After pre-processing the dataset, let's recover the final character set used to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpHk6HW6O0FW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_manifest_data = read_manifest(train_manifest_cleaned)\n",
    "train_charset = get_charset(train_manifest_data)\n",
    "\n",
    "#dev_manifest_data = read_manifest(dev_manifest_cleaned)\n",
    "#dev_charset = get_charset(dev_manifest_data)\n",
    "\n",
    "train_dev_set = set.union(set(train_charset.keys())) #, set(dev_charset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3xkR4_dPd3C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of tokens in preprocessed train+dev set : {len(train_dev_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnmVqx8aegwR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Character Encoding CTC Model\n",
    "\n",
    "Now that we have a processed dataset, we can begin training an ASR model on this dataset. The following section will detail how we prepare a CTC model which utilizes a Character Encoding scheme.\n",
    "\n",
    "This section will utilize a pre-trained [QuartzNet 15x5](https://arxiv.org/abs/1910.10261), which has been trained on roughly 7,000 hours of English speech base model. We will modify the decoder layer (thereby changing the model's vocabulary) and then train for a small number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlJmwh-iei77",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "char_model = nemo_asr.models.ASRModel.from_pretrained(\"stt_en_quartznet15x5\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSI6t9dgSOxj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Update the vocabulary\n",
    "\n",
    "Changing the vocabulary of a character encoding ASR model is as simple as passing the list of new tokens that comprise the vocabulary as input to `change_vocabulary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VU-jfYLei9-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "char_model.change_vocabulary(new_vocabulary=list(train_dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(train_dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PPDTaLyejAR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Freeze Encoder { display-mode: \"form\" }\n",
    "freeze_encoder = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
    "freeze_encoder = bool(freeze_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qiTTgDGejC9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def enable_bn_se(m):\n",
    "    if type(m) == nn.BatchNorm1d:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    if 'SqueezeExcite' in type(m).__name__:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9I5dx_GWejFm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if freeze_encoder:\n",
    "  char_model.encoder.freeze()\n",
    "  char_model.encoder.apply(enable_bn_se)\n",
    "  logging.info(\"Model encoder has been frozen, and batch normalization has been unfrozen\")\n",
    "else:\n",
    "  char_model.encoder.unfreeze()\n",
    "  logging.info(\"Model encoder has been un-frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_cAiuXSfRdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Update config\n",
    "\n",
    "Each NeMo model has a config embedded in it, which can be accessed via `model.cfg`. In general, this is the config that was used to construct the model.\n",
    "\n",
    "For pre-trained models, this config generally represents the config used to construct the model when it was trained. A nice benefit to this embedded config is that we can repurpose it to set up new data loaders, optimizers, schedulers, and even data augmentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eklNZ4ynWhbB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Updating the character set of the model\n",
    "\n",
    "The most important step for preparing character encoding models for fine-tuning is to update the model's character set. Remember - the model was trained on some language with some specific dataset that had a certain character set. Character sets would rarely remain the same between training and fine-tuning (though it is still possible).\n",
    "\n",
    "Each character encoding model has a `model.cfg.labels` attribute, which can be overridden via OmegaConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBIy8p0fV7sa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "char_model.cfg.labels = list(train_dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEEzZD4gXGmm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we create a working copy of the model config and update it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzpByrdfejIA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg = copy.deepcopy(char_model.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBFvo0UUfcuY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up data loaders\n",
    "\n",
    "Now that the model's character set has been updated let's prepare the model to utilize the new character set even in the data loaders. Note that this is crucial so that the data produced during training/validation matches the new character set, and tokens are encoded/decoded correctly.\n",
    "\n",
    "**Note**: An important config parameter is `normalize_transcripts` and `parser`. There are some parsers that are used for specific languages for character based models - currently only `en` is supported. These parsers will preprocess the text with the given languages parser. However, for other languages, it is advised to explicitly set `normalize_transcripts = False` - which will prevent the parser from processing text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlQ5iGrZejKy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup train, validation, test configs\n",
    "\n",
    "with open_dict(cfg):    \n",
    "  # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
    "  cfg.train_ds.manifest_filepath = f\"{train_manifest_cleaned}\" #\",{dev_manifest_cleaned}\"\n",
    "  cfg.train_ds.labels = list(train_dev_set)\n",
    "  cfg.train_ds.normalize_transcripts = False\n",
    "  cfg.train_ds.batch_size = 32\n",
    "  cfg.train_ds.num_workers = 8\n",
    "  cfg.train_ds.pin_memory = True\n",
    "  cfg.train_ds.trim_silence = True\n",
    "\n",
    "  # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
    "  cfg.validation_ds.manifest_filepath = test_manifest_cleaned\n",
    "  cfg.validation_ds.labels = list(train_dev_set)\n",
    "  cfg.validation_ds.normalize_transcripts = False\n",
    "  cfg.validation_ds.batch_size = 8\n",
    "  cfg.validation_ds.num_workers = 8\n",
    "  cfg.validation_ds.pin_memory = True\n",
    "  cfg.validation_ds.trim_silence = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tx9DixV0ejMo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# setup data loaders with new configs\n",
    "char_model.setup_training_data(cfg.train_ds)\n",
    "char_model.setup_multiple_validation_data(cfg.validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJc7DyEUfem2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up optimizer and scheduler\n",
    "\n",
    "When fine-tuning character models, it is generally advised to use a lower learning rate and reduced warmup. A reduced learning rate helps preserve the pre-trained weights of the encoder. Since the fine-tuning dataset is generally smaller than the original training dataset, the warmup steps would be far too much for the smaller fine-tuning dataset.\n",
    "\n",
    "-----\n",
    "**Note**: When freezing the encoder, it is possible to use the original learning rate as the model was trained on. The original learning rate can be used because the encoder is frozen, so the learning rate is used only to optimize the decoder. However, a very high learning rate would still destabilize training, even with a frozen encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgoD5hOKYSKJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Original optimizer + scheduler\n",
    "print(OmegaConf.to_yaml(char_model.cfg.optim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okytaslHejOm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open_dict(char_model.cfg.optim):\n",
    "  char_model.cfg.optim.lr = 0.0004 #0.01 for freezing\n",
    "  char_model.cfg.optim.betas = [0.95, 0.5]  # from paper\n",
    "  char_model.cfg.optim.weight_decay = 0.001  # Original weight decay\n",
    "  char_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
    "  char_model.cfg.optim.sched.warmup_ratio = 0.05  # 5 % warmup\n",
    "  char_model.cfg.optim.sched.min_lr = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Waz64_NXfkIQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting up augmentation\n",
    "\n",
    "Remember that the model was trained on several thousands of hours of data, so the regularization provided to it might not suit the current dataset. We can easily change it as we see fit.\n",
    "\n",
    "-----\n",
    "\n",
    "You might notice that we utilize `char_model.from_config_dict()` to create a new SpectrogramAugmentation object and assign it directly in place of the previous augmentation. This is generally the syntax to be followed whenever you notice a `_target_` tag in the config of a model's inner config. \n",
    "\n",
    "-----\n",
    "**Note**: For low resource languages, it might be better to increase augmentation via SpecAugment to reduce overfitting. However, this might, in turn, make it too hard for the model to train in a short number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ6Md-dLejRA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(char_model.cfg.spec_augment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ei9WsLzejTI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#with open_dict(char_model.cfg.spec_augment):\n",
    "#   char_model.cfg.spec_augment.freq_masks = 2\n",
    "#   char_model.cfg.spec_augment.freq_width = 25\n",
    "#   char_model.cfg.spec_augment.time_masks = 2\n",
    "#   char_model.cfg.spec_augment.time_width = 0.05\n",
    "\n",
    "char_model.spec_augmentation = char_model.from_config_dict(char_model.cfg.spec_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK-RQXEZfq1V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Metrics\n",
    "\n",
    "Originally, the model was trained on an English dataset corpus. When calculating Word Error Rate, we can easily use the \"space\" token as a separator for word boundaries. On the other hand, certain languages such as Japanese and Mandarin do not use \"space\" tokens, instead opting for different ways to annotate the end of the word.\n",
    "\n",
    "In cases where the \"space\" token is not used to denote a word boundary, we can use the Character Error Rate metric instead, which computes the edit distance at a token level rather than a word level.\n",
    "\n",
    "We might also be interested in noting model predictions during training and inference. As such, we can enable logging of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cN1FC0o2ejVg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Metric\n",
    "use_cer = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
    "log_prediction = True #@param [\"False\", \"True\"] {type:\"raw\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HURZMpPwejXa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "char_model._wer.use_cer = use_cer\n",
    "char_model._wer.log_prediction = log_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rizGRfrHf92O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Trainer and Experiment Manager\n",
    "\n",
    "And that's it! Now we can train the model by simply using the Pytorch Lightning Trainer and NeMo Experiment Manager as always.\n",
    "\n",
    "For demonstration purposes, the number of epochs is kept intentionally low. Reasonable results can be obtained in around 100 epochs (approximately 25 minutes on Colab GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaw1qsQIf1Zv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as ptl\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  accelerator = 'gpu'\n",
    "else:\n",
    "  accelerator = 'cpu'\n",
    "\n",
    "EPOCHS = 20  # 100 epochs would provide better results, but would take an hour to train\n",
    "\n",
    "#checkpoint_callback = ModelCheckpoint(\n",
    "#    filepath=os.path.join('checkpoints', '{epoch:d}'),\n",
    "#    verbose=True,\n",
    "#    save_last=True,\n",
    "#    save_top_k=args.save_top_k,\n",
    "#    monitor='val_acc',\n",
    "#    mode='max'\n",
    "#)\n",
    "trainer = ptl.Trainer(gpus=[1], \n",
    "                      accelerator=accelerator, \n",
    "                      max_epochs=EPOCHS, \n",
    "                      accumulate_grad_batches=1,\n",
    "                      enable_checkpointing=True,\n",
    "                      logger=False,\n",
    "                      log_every_n_steps=500,\n",
    "                      #callbacks = [checkpoint_callback],\n",
    "                      check_val_every_n_epoch=1)\n",
    "# resume_from_checkpoint\n",
    "# Setup model with the trainer\n",
    "char_model.set_trainer(trainer)\n",
    "\n",
    "# Finally, update the model's internal config\n",
    "char_model.cfg = char_model._cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENSpJJqcf1cG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Environment variable generally used for multi-node multi-gpu training.\n",
    "# In notebook environments, this flag is unnecessary and can cause logs of multiple training runs to overwrite each other.\n",
    "os.environ.pop('NEMO_EXPM_VERSION', None)\n",
    "\n",
    "config = exp_manager.ExpManagerConfig(\n",
    "    exp_dir=f'experiments/lang-{LANGUAGE}/',\n",
    "    name=f\"ASR-Char-Model-Language-{LANGUAGE}\",\n",
    "    create_checkpoint_callback=False,\n",
    "    checkpoint_callback_params=exp_manager.CallbackParams(\n",
    "        monitor=\"val_wer\",\n",
    "        mode=\"min\",\n",
    "        always_save_nemo=True,\n",
    "        save_best_model=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "config = OmegaConf.structured(config)\n",
    "\n",
    "logdir = exp_manager.exp_manager(trainer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATI2R0D7rylR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google import colab\n",
    "  COLAB_ENV = True\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "  COLAB_ENV = False\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "if COLAB_ENV:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir /content/experiments/lang-$LANGUAGE/ASR-Char-Model-Language-$LANGUAGE/\n",
    "else:\n",
    "  print(\"To use tensorboard, please use this notebook in a Google Colab environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvaESyJHf1eb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.fit(char_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "timestamp = dt.datetime.now().strftime(\"%H-%M-%d-%B\")\n",
    "save_path = f\"/mnt/sdb/jhchang/nemo/Model-{LANGUAGE}-{EPOCHS}-{timestamp}.nemo\"\n",
    "char_model.save_to(f\"{save_path}\")\n",
    "print(f\"Model saved at path : {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer = ptl.Trainer(gpus=[1], \n",
    "                      accelerator=accelerator, \n",
    "                      max_epochs=4, \n",
    "                      accumulate_grad_batches=1,\n",
    "                      enable_checkpointing=True,\n",
    "                      logger=False,\n",
    "                      log_every_n_steps=5000,\n",
    "                      #callbacks = [checkpoint_callback],\n",
    "                      check_val_every_n_epoch=1)\n",
    "trainer.fit(char_model)\n",
    "\n",
    "import datetime as dt\n",
    "timestamp = dt.datetime.now().strftime(\"%H-%M-%d-%B\")\n",
    "save_path = f\"/mnt/sdb/jhchang/nemo/Model-{LANGUAGE}-{trainer.max_epochs}-{timestamp}.nemo\"\n",
    "char_model.save_to(f\"{save_path}\")\n",
    "print(f\"Model saved at path : {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bigger batch-size = bigger throughput\n",
    "#params['char_model']['validation_ds']['batch_size'] = 16\n",
    "# Setup the test data loader and make sure the model is on GPU\n",
    "\n",
    "char_model = nemo_asr.models.ASRModel.restore_from(f'/mnt/sdb/jhchang/nemo/Model-ko-epoch 20-05-11-06-July_WER(0.0438).nemo')\n",
    "cfg = copy.deepcopy(char_model.cfg)\n",
    "\n",
    "with open_dict(cfg):\n",
    "  # Train dataset  (Concatenate train manifest cleaned and dev manifest cleaned)\n",
    "  cfg.train_ds.manifest_filepath = f\"{train_manifest_cleaned}\" #\",{dev_manifest_cleaned}\"\n",
    "  cfg.train_ds.labels = None\n",
    "  cfg.train_ds.normalize_transcripts = False\n",
    "  cfg.train_ds.batch_size = 32\n",
    "  cfg.train_ds.num_workers = 8\n",
    "  cfg.train_ds.pin_memory = True\n",
    "  cfg.train_ds.trim_silence = True\n",
    "\n",
    "  # Validation dataset  (Use test dataset as validation, since we train using train + dev)\n",
    "  cfg.validation_ds.manifest_filepath = test_manifest_cleaned\n",
    "  cfg.validation_ds.labels = None\n",
    "  cfg.validation_ds.normalize_transcripts = False\n",
    "  cfg.validation_ds.batch_size = 8\n",
    "  cfg.validation_ds.num_workers = 8\n",
    "  cfg.validation_ds.pin_memory = True\n",
    "  cfg.validation_ds.trim_silence = True\n",
    "\n",
    "char_model.setup_test_data(cfg.validation_ds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_CTC_Language_Finetuning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}